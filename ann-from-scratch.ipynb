{"cells":[{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T13:05:28.224770Z","iopub.status.busy":"2024-09-14T13:05:28.223362Z","iopub.status.idle":"2024-09-14T13:05:33.487857Z","shell.execute_reply":"2024-09-14T13:05:33.486647Z","shell.execute_reply.started":"2024-09-14T13:05:28.224718Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Size: (33600, 784)\n","Validation Size: (8400, 784)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","# Load the data\n","train_data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n","test_data = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n","X_train = train_data.iloc[:, 1:]  # Removing Labels\n","Y_train = train_data['label']  # Extracting Y labels\n","X_test = test_data.iloc[:, 1:]\n","\n","# Split the data into training and validation sets\n","X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42,shuffle=True)\n","\n","if __name__ == \"__main__\":\n","    print(\"Train Size:\", X_train.shape)\n","    print(\"Validation Size:\", X_val.shape)\n"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T13:10:50.561314Z","iopub.status.busy":"2024-09-14T13:10:50.560766Z","iopub.status.idle":"2024-09-14T13:10:50.610828Z","shell.execute_reply":"2024-09-14T13:10:50.607799Z","shell.execute_reply.started":"2024-09-14T13:10:50.561264Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def ReLU(Z):\n","    return np.maximum(0, Z)\n","\n","def SoftMax(Z): \n","    Z_shifted = Z - np.max(Z, axis=0, keepdims=True) \n","    expZ = np.exp(Z_shifted)\n","    return expZ / np.sum(expZ, axis=0, keepdims=True)\n","\n","def one_hot(Y):\n","    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n","    one_hot_Y[np.arange(Y.size), Y] = 1\n","    return one_hot_Y.T\n","\n","def ReLU_deriv(Z):\n","    return Z > 0\n","    \n","class MNIST_model():\n","    def __init__(self):\n","        # Initializing weights\n","        self.W1 = np.random.rand(224, 784) - 0.5\n","        self.b1 = np.random.rand(224, 1) - 0.5\n","        self.W2 = np.random.rand(10, 224) - 0.5\n","        self.b2 = np.random.rand(10, 1) - 0.5\n","        \n","        # Adam optimizer parameters\n","        self.mW1, self.vW1 = np.zeros_like(self.W1), np.zeros_like(self.W1)\n","        self.mb1, self.vb1 = np.zeros_like(self.b1), np.zeros_like(self.b1)\n","        self.mW2, self.vW2 = np.zeros_like(self.W2), np.zeros_like(self.W2)\n","        self.mb2, self.vb2 = np.zeros_like(self.b2), np.zeros_like(self.b2)\n","        self.epsilon = 1e-8\n","        \n","    def forward(self, X):\n","        self.Z1 = self.W1.dot(X) + self.b1\n","        self.A1 = ReLU(self.Z1)\n","        self.Z2 = self.W2.dot(self.A1) + self.b2\n","        self.A2 = SoftMax(self.Z2)\n","        return self.A2\n","    \n","    def backprop(self, Y, X):\n","        m = Y.size\n","        self.one_hot_Y = one_hot(Y)\n","        self.dZ2 = self.A2 - self.one_hot_Y\n","        self.dW2 = 1 / m * self.dZ2.dot(self.A1.T)\n","        self.db2 = 1 / m * np.sum(self.dZ2, axis=1, keepdims=True) \n","        self.dZ1 = self.W2.T.dot(self.dZ2) * ReLU_deriv(self.Z1)\n","        self.dW1 = 1 / m * self.dZ1.dot(X.T)\n","        self.db1 = 1 / m * np.sum(self.dZ1, axis=1, keepdims=True)\n","        \n","    def get_predictions(self, A2):\n","        return np.argmax(A2, axis=0)\n","    \n","    def learning_rate_decay(self, lr, t, decay_rate):\n","        self.lr = lr * (1 / (1 + t * decay_rate))\n","        \n","    def get_accuracy(self, predictions, Y):\n","        return np.sum(predictions == Y) / Y.size\n","    \n","    def adam_optimizer(self, param, grad, m, v, beta1, beta2, lr, t):\n","        m = beta1 * m + (1 - beta1) * grad\n","        v = beta2 * v + (1 - beta2) * (grad ** 2)\n","        m_hat = m / (1 - beta1 ** t)\n","        v_hat = v / (1 - beta2 ** t)\n","        param -= lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n","        return param, m, v\n","    \n","    def update_param(self, lr, beta1=0.9, beta2=0.999):\n","        self.W1, self.mW1, self.vW1 = self.adam_optimizer(self.W1, self.dW1, self.mW1, self.vW1, beta1, beta2, lr, self.iteration)\n","        self.b1, self.mb1, self.vb1 = self.adam_optimizer(self.b1, self.db1, self.mb1, self.vb1, beta1, beta2, lr, self.iteration)\n","        self.W2, self.mW2, self.vW2 = self.adam_optimizer(self.W2, self.dW2, self.mW2, self.vW2, beta1, beta2, lr, self.iteration)\n","        self.b2, self.mb2, self.vb2 = self.adam_optimizer(self.b2, self.db2, self.mb2, self.vb2, beta1, beta2, lr, self.iteration)\n","        \n","    def train(self, iterations, X, Y, lr):\n","        self.iteration = 1\n","        for i in range(iterations):\n","            A2 = self.forward(X)\n","            self.backprop(Y, X)\n","            self.update_param(lr)\n","            \n","            if i % 10 == 0:\n","                self.train_predictions = self.get_predictions(A2)\n","                print(f'Iteration: {i} \\nTrain Accuracy: {int(self.get_accuracy(self.train_predictions, Y) * 100)}%')\n","                self.check_accuracy(X_val.T,Y_val)\n","            self.iteration += 1\n","    \n","    def check_accuracy(self, X, Y):\n","        A2 = self.forward(X)\n","        test_predictions = self.get_predictions(A2)\n","        print(f'Test Accuracy: {int(self.get_accuracy(test_predictions, Y) * 100)}%\\n')\n","    \n","    def make_predictions(self, X):\n","        A2 = self.forward(X)\n","        predictions = self.get_predictions(A2)\n","        return predictions\n","\n","    def test_prediction(self, index, X_train, Y_train):\n","        current_image = X_train[:, index, None]\n","        prediction = self.make_predictions(current_image)\n","        label = Y_train[index]\n","        print(\"Prediction: \", prediction)\n","        print(\"Label: \", label)\n","\n","        current_image = current_image.reshape((28, 28)) * 255\n","        plt.gray()\n","        plt.imshow(current_image, interpolation='nearest')\n","        plt.show()\n"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T13:11:36.696319Z","iopub.status.busy":"2024-09-14T13:11:36.695827Z","iopub.status.idle":"2024-09-14T13:13:11.962335Z","shell.execute_reply":"2024-09-14T13:13:11.960532Z","shell.execute_reply.started":"2024-09-14T13:11:36.696277Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 0 \n","Train Accuracy: 11%\n","Test Accuracy: 13%\n","\n","Iteration: 10 \n","Train Accuracy: 80%\n","Test Accuracy: 82%\n","\n","Iteration: 20 \n","Train Accuracy: 87%\n","Test Accuracy: 86%\n","\n","Iteration: 30 \n","Train Accuracy: 89%\n","Test Accuracy: 88%\n","\n","Iteration: 40 \n","Train Accuracy: 90%\n","Test Accuracy: 89%\n","\n","Iteration: 50 \n","Train Accuracy: 91%\n","Test Accuracy: 90%\n","\n","Iteration: 60 \n","Train Accuracy: 92%\n","Test Accuracy: 90%\n","\n","Iteration: 70 \n","Train Accuracy: 92%\n","Test Accuracy: 91%\n","\n","Iteration: 80 \n","Train Accuracy: 93%\n","Test Accuracy: 91%\n","\n","Iteration: 90 \n","Train Accuracy: 94%\n","Test Accuracy: 91%\n","\n"]}],"source":["model = MNIST_model()\n","model.train(100,X_train.T,Y_train,0.01)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":861823,"sourceId":3004,"sourceType":"competition"},{"datasetId":366471,"sourceId":714968,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
